{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email_Spam_Classification_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHXBIxwLAMNL",
        "outputId": "b3b2c972-63f2-446e-c4e1-d52dff5af83e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import gensim\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import csr_matrix\n",
        "from scipy.sparse import coo_matrix,hstack\n",
        "from tqdm import tqdm\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextFeaturization():\n",
        "\n",
        "  def extract_email_ids(self,doc):\n",
        "    '''This functions extract the email ids and domain names in the email adderss and returns a list of preprocessed email ids'''\n",
        "    list_of_preproessed_emails = []\n",
        "    list_of_emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',doc)\n",
        "    doc = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\" \",doc)\n",
        "    for txt in list_of_emails:\n",
        "      email = re.split(\"[.]\",re.split(\"@\", txt)[1])\n",
        "      y=email.copy()\n",
        "      for i in email:\n",
        "        if i==\"com\" or len(i)<=2:\n",
        "          y.remove(i)\n",
        "      email = ' '.join([str(i) for i in y])\n",
        "      email = email.lower()\n",
        "      list_of_preproessed_emails.append(email)\n",
        "    list_of_preproessed_emails = \" \".join(list_of_preproessed_emails)\n",
        "    return list_of_preproessed_emails\n",
        "\n",
        "  def text_lowercase(self,doc):\n",
        "    ''' This function converts the text to lower case'''\n",
        "    return doc.lower()\n",
        "\n",
        "  def remove_digits(self, doc):\n",
        "    '''This function removes all the numbers'''\n",
        "    return re.sub('\\d', '', doc)\n",
        "\n",
        "  def remove_underscores(self, doc):\n",
        "    '''This function removes all the underscores'''\n",
        "    return re.sub(r'_', '', doc)\n",
        "\n",
        "  def remove_excess_whitespace(self, doc):\n",
        "    '''This function removes excess white spaces'''\n",
        "    return re.sub('\\s+', ' ', doc)\n",
        "\n",
        "  def remove_special_characters(self, doc):\n",
        "    '''This function removes all the special characters'''\n",
        "    return re.sub('\\W', ' ', doc)\n",
        "\n",
        "  def remove_within_brackets(self, doc):\n",
        "    '''This function removes all the content within brackets'''\n",
        "    text = re.sub(r'\\([^()]*\\)', '', doc)\n",
        "    text = re.sub(r'<[^()]*>', '', text)\n",
        "    return text\n",
        "\n",
        "  def expand_words(self, phrase):\n",
        "    '''This function expands the short form words '''\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "  def remove_short_and_long_words(self, doc):\n",
        "    '''This function removes all the short(<2 letters) and long(>15 letters) words '''\n",
        "    words = doc.split()\n",
        "    word_list = []\n",
        "    for word in words:\n",
        "      if len(word) > 2 and len(word) < 15 :\n",
        "        word_list.append(word)\n",
        "    return ' '.join(word_list)\n",
        "\n",
        "  \n",
        "\n",
        "  def text_lematizer(self,doc):\n",
        "    '''This function lematize the words to its root words'''\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    doc = nlp(doc)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "  def clean_document(self,doc):\n",
        "    '''This function cleans the documents'''\n",
        "    doc = self.text_lowercase(doc)\n",
        "    ids = self.extract_email_ids(doc)\n",
        "    doc = self.remove_within_brackets(doc)\n",
        "    doc = self.expand_words(doc)\n",
        "    doc = self.remove_underscores(doc)\n",
        "    doc = self.remove_special_characters(doc)\n",
        "    doc = self.remove_digits(doc)\n",
        "    doc = self.remove_excess_whitespace(doc)\n",
        "    doc = self.remove_short_and_long_words(doc)\n",
        "    doc = self.text_lematizer(doc)\n",
        "    doc = ids+doc\n",
        "    return doc\n",
        "  \n",
        "\n",
        "  def remove_stopwords(self,doc,stopword_list):\n",
        "    '''This function removes the stop words'''\n",
        "    word_list = doc.split(\" \")\n",
        "    cleaned_txt = [w for w in word_list if not w in stopword_list]\n",
        "    cleaned_string = \" \".join(cleaned_txt)\n",
        "    \n",
        "    return cleaned_string\n",
        "    \n",
        "  def word_count(self,doc):#need to change to corpus\n",
        "    '''This function retuns an array of word count in each document'''\n",
        "    return len(doc.split())\n",
        "\n",
        "  def average_word_length(self,doc):\n",
        "    '''This function returns an array of average word length in each document'''\n",
        "    total_length = 0\n",
        "    for i in doc.split():\n",
        "      total_length += len(i)\n",
        "    return (total_length/len(doc.split()))\n",
        "\n",
        "\n",
        "  def tfidf_test(self,data):\n",
        "    '''This function creates TFIDF representation of test data'''\n",
        "    tfidf_model = pickle.load(open('tfidf_model.sav', 'rb'))\n",
        "    return tfidf_model.transform([data])\n",
        "\n",
        "\n",
        "  def perform_lda(self,data):\n",
        "    '''This function performs lda on test data'''\n",
        "    vectorizer_bow = pickle.load(open('bow_model.sav', 'rb'))\n",
        "    lda_model = pickle.load(open('lda_model.sav', 'rb'))\n",
        "    bow = vectorizer_bow.transform([data])\n",
        "    return lda_model.transform(bow)\n",
        "\n",
        "  def featurize(self,X_test,email_stopwords):\n",
        "    '''This function featurize text data as w2V,word length,avg word length,lda topic modelling for given train and test data'''\n",
        "    email_data = []\n",
        "    test_doc = self.remove_stopwords(X_test,email_stopwords)\n",
        "\n",
        "    word_count_test = self.word_count(test_doc)\n",
        "    avg_word_len_test = self.average_word_length(test_doc)\n",
        "    lda_test = self.perform_lda(test_doc)\n",
        "    tfidf_test_vec = self.tfidf_test(test_doc)\n",
        "    email_data.extend([word_count_test,avg_word_len_test])\n",
        "    email_data.extend(lda_test[0])\n",
        "    email_data.extend(np.asarray(tfidf_test_vec.todense()[0])[0])\n",
        "    return np.array(email_data).reshape(1, -1)\n"
      ],
      "metadata": {
        "id": "0wmhL-goDdzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailClassification(TextFeaturization):\n",
        "\n",
        "  def classify(self,data):\n",
        "    with open('stopword.pkl', 'rb') as f:\n",
        "      stopwords = pickle.load(f) \n",
        "    data = self.clean_document(data)\n",
        "    data = self.featurize(data,stopwords)\n",
        "    clf = pickle.load(open('rf_model.sav', 'rb'))\n",
        "    return (\"SPAM\" if clf.predict(data)==1 else \"HAM\")"
      ],
      "metadata": {
        "id": "mTudZoxODfmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = '''From rssfeeds@jmason.org Fri Oct 4 11:02:10 2002 Return-Path: <rssfeeds@spamassassin.taint.org> Delivered-To: yyyy@localhost.spamassassin.taint.org Received: from localhost (jalapeno [127.0.0.1]) by jmason.org (Postfix) with ESMTP id 3C0DA16F6F for <jm@localhost>; Fri, 4 Oct 2002 11:01:47 +0100 (IST) Received: from jalapeno [127.0.0.1] by localhost with IMAP (fetchmail-5.9.0) for jm@localhost (single-drop); Fri, 04 Oct 2002 11:01:47 +0100 (IST) Received: from dogma.slashnull.org (localhost [127.0.0.1]) by dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g9480QK08803 for <jm@jmason.org>; Fri, 4 Oct 2002 09:00:26 +0100 Message-Id: <200210040800.g9480QK08803@dogma.slashnull.org> To: yyyy@spamassassin.taint.org From: boingboing <rssfeeds@spamassassin.taint.org> Subject: Don't do the brown WiFi, the brown WiFi is BAD Date: Fri, 04 Oct 2002 08:00:25 -0000 Content-Type: text/plain; encoding=utf-8 URL: http://boingboing.net/#85515860 Date: Not supplied Rob \"Pringles Can\" Flickenger and others Cliff Skolnik at the O'Reilly OS X con has tracked down the cause of the annoying flakiness in the wireless network here -- every 20 or 30 seconds, you start getting \"connection refused\" messages from your browser and other net-utilities. Rob \"Pringles Can\" Flickenger wrote it up. It turns out that running the great network-spy app Etherpeg[1] (or other \"promiscuous\" network sniffers) and the built-in firewall in OS X at the same time causes your computer to begin intercepting every packet sent out on your segment of the wireless network and respond to it with a \"rejected\" message. So today, Rob (and everyone else who knows about this) is going to run around and tell people running Etherpeg to _turn off the firewall_ (and vice-versa). Ah, fickle networking, you are such a stern mistress! Link[2] Discuss[3] (_ Thanks, Rob!_) [1] http://www.oreillynet.com/pub/wlg/1414 [2] http://www.oreillynet.com/pub/wlg/2086 [3] http://www.quicktopic.com/boing/H/bfYib9hETQSA'''"
      ],
      "metadata": {
        "id": "G_n9snov6gft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = EmailClassification()\n",
        "clf.classify(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aKlC6z286XaE",
        "outputId": "321c68c9-2335-4aeb-ec86-5297fe5e2646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HAM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}